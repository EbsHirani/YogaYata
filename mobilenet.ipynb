{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mobilenet.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "in_kg8GD2FWU"
      },
      "source": [
        "import keras\r\n",
        "from keras import backend as K\r\n",
        "from keras.layers.core import Dense, Activation\r\n",
        "from keras.optimizers import Adam\r\n",
        "from keras.metrics import categorical_crossentropy\r\n",
        "from keras.preprocessing.image import ImageDataGenerator\r\n",
        "from keras.preprocessing import image\r\n",
        "from keras.models import Model\r\n",
        "from keras.applications import imagenet_utils\r\n",
        "from keras.layers import Dense,GlobalAveragePooling2D\r\n",
        "from keras.applications import MobileNetV2\r\n",
        "from keras.applications.mobilenet import preprocess_input\r\n",
        "import numpy as np\r\n",
        "from IPython.display import Image\r\n",
        "from keras.optimizers import Adam\r\n",
        "from PIL import ImageFile\r\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvrlArYE1_DZ"
      },
      "source": [
        "base_model=MobileNetV2(weights='imagenet',include_top=False, input_shape= (224,224,3)) #imports the mobilenet model and discards the last 1000 neuron layer.\r\n",
        "base_model.trainable = False\r\n",
        "inp = keras.layers.Input(shape=(224,224,3))\r\n",
        "x = keras.layers.experimental.preprocessing.Rescaling(\r\n",
        "    1/255.\r\n",
        ")(inp)\r\n",
        "\r\n",
        "x=base_model(x)\r\n",
        "x=GlobalAveragePooling2D()(x)\r\n",
        "x=Dense(512,activation='relu')(x) #dense layer 3\r\n",
        "preds=Dense(5,activation='softmax')(x) #final layer with softmax activation"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Mj138Ej2K-3"
      },
      "source": [
        "model=Model(inputs = inp,outputs=preds)\r\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['acc'])"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IspJoLEFDy9W",
        "outputId": "5c2a19a1-b3fc-477c-df27-3b80d85d177b"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_19 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
            "_________________________________________________________________\n",
            "rescaling_6 (Rescaling)      (None, 224, 224, 3)       0         \n",
            "_________________________________________________________________\n",
            "mobilenetv2_1.00_224 (Functi (None, 7, 7, 1280)        2257984   \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_4 ( (None, 1280)              0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 512)               655872    \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 5)                 2565      \n",
            "=================================================================\n",
            "Total params: 2,916,421\n",
            "Trainable params: 658,437\n",
            "Non-trainable params: 2,257,984\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQEXv4nO2wap",
        "outputId": "d2e31f45-5e8d-4c87-df92-c0504cd980fd"
      },
      "source": [
        "img_dir = \"/content/drive/MyDrive/Datasets/DATASET\"\r\n",
        "batch_size=32\r\n",
        "image_size=(224, 224)\r\n",
        "\r\n",
        "datagen_cat = ImageDataGenerator(\r\n",
        "   \r\n",
        ")\r\n",
        "\r\n",
        "train_gen_cat = datagen_cat.flow_from_directory(\r\n",
        "    img_dir+\"/TRAIN\",\r\n",
        "    seed=1337,\r\n",
        "    shuffle=True,\r\n",
        "    target_size=image_size,\r\n",
        "    batch_size=batch_size,\r\n",
        ")\r\n",
        "\r\n",
        "val_gen_cat = datagen_cat.flow_from_directory(\r\n",
        "    img_dir+\"/TEST\",\r\n",
        "    seed=1337,\r\n",
        "    shuffle=True,\r\n",
        "    target_size=image_size,\r\n",
        "    batch_size=batch_size,\r\n",
        ")"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1081 images belonging to 5 classes.\n",
            "Found 470 images belonging to 5 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2Vfaw164KG_"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\r\n",
        "Patience=2\r\n",
        "BestModelWeightsPath = \"{}_weights.best.h5\".format(model)\r\n",
        "checkpoint = ModelCheckpoint(\r\n",
        "    BestModelWeightsPath, \r\n",
        "    monitor='val_acc', \r\n",
        "    verbose=1, \r\n",
        "    save_best_only=True, \r\n",
        "    mode='max', \r\n",
        "    save_weights_only = True,\r\n",
        ")\r\n",
        "reduceLROnPlat = ReduceLROnPlateau(\r\n",
        "    monitor='val_acc', \r\n",
        "    factor=0.2, \r\n",
        "    patience=Patience, \r\n",
        "    verbose=1, \r\n",
        "    mode='max', \r\n",
        "    cooldown=2, \r\n",
        "    min_lr=0.000001\r\n",
        ")\r\n",
        "early = EarlyStopping(\r\n",
        "    monitor=\"val_acc\", \r\n",
        "    mode=\"max\", \r\n",
        "    patience=Patience*2,\r\n",
        "    restore_best_weights=True\r\n",
        ")\r\n",
        "callbacks_list_final= [checkpoint, early, reduceLROnPlat]"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ahsUM0wg4tgM",
        "outputId": "c1383b62-d406-4035-c11b-272e4b837098"
      },
      "source": [
        "classification = model.fit(train_gen_cat,validation_data=val_gen_cat,epochs=100,verbose=1,callbacks=callbacks_list_final)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            " 2/34 [>.............................] - ETA: 10s - loss: 2.0149 - acc: 0.2804 "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "34/34 [==============================] - ETA: 0s - loss: 1.2111 - acc: 0.5763"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/TiffImagePlugin.py:770: UserWarning: Possibly corrupt EXIF data.  Expecting to read 11 bytes but only got 10. Skipping tag 42037\n",
            "  \" Skipping tag %s\" % (size, len(data), tag)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r34/34 [==============================] - 38s 1s/step - loss: 1.1994 - acc: 0.5804 - val_loss: 0.2407 - val_acc: 0.9234\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.92340, saving model to <tensorflow.python.keras.engine.functional.Functional object at 0x7fab8bbce6d0>_weights.best.h5\n",
            "Epoch 2/100\n",
            "34/34 [==============================] - 34s 1000ms/step - loss: 0.2393 - acc: 0.9355 - val_loss: 0.2073 - val_acc: 0.9362\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.92340 to 0.93617, saving model to <tensorflow.python.keras.engine.functional.Functional object at 0x7fab8bbce6d0>_weights.best.h5\n",
            "Epoch 3/100\n",
            "34/34 [==============================] - 33s 993ms/step - loss: 0.1373 - acc: 0.9734 - val_loss: 0.1838 - val_acc: 0.9468\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.93617 to 0.94681, saving model to <tensorflow.python.keras.engine.functional.Functional object at 0x7fab8bbce6d0>_weights.best.h5\n",
            "Epoch 4/100\n",
            "34/34 [==============================] - 34s 1s/step - loss: 0.0608 - acc: 0.9960 - val_loss: 0.1853 - val_acc: 0.9447\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.94681\n",
            "Epoch 5/100\n",
            "34/34 [==============================] - 34s 1s/step - loss: 0.0388 - acc: 0.9956 - val_loss: 0.1713 - val_acc: 0.9553\n",
            "\n",
            "Epoch 00005: val_acc improved from 0.94681 to 0.95532, saving model to <tensorflow.python.keras.engine.functional.Functional object at 0x7fab8bbce6d0>_weights.best.h5\n",
            "Epoch 6/100\n",
            "34/34 [==============================] - 34s 996ms/step - loss: 0.0190 - acc: 0.9996 - val_loss: 0.1998 - val_acc: 0.9404\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.95532\n",
            "Epoch 7/100\n",
            "34/34 [==============================] - 33s 996ms/step - loss: 0.0211 - acc: 0.9980 - val_loss: 0.1931 - val_acc: 0.9404\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.95532\n",
            "\n",
            "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
            "Epoch 8/100\n",
            "34/34 [==============================] - 34s 987ms/step - loss: 0.0238 - acc: 0.9989 - val_loss: 0.1707 - val_acc: 0.9532\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.95532\n",
            "Epoch 9/100\n",
            "34/34 [==============================] - 33s 982ms/step - loss: 0.0089 - acc: 0.9999 - val_loss: 0.1725 - val_acc: 0.9532\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.95532\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SU41NvYn6AV9",
        "outputId": "8b6b1058-c312-4d5b-835f-230c71389253"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\r\n",
        "tflite_model = converter.convert()\r\n",
        "with open('model.tflite', 'wb') as f:\r\n",
        "  f.write(tflite_model)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmppztl696m/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmppztl696m/assets\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_UXkfyO43b4",
        "outputId": "fdcb5558-6539-4f74-f470-3169f4ec2877"
      },
      "source": [
        "import numpy as np\r\n",
        "import tensorflow as tf\r\n",
        "import cv2\r\n",
        "# Load TFLite model and allocate tensors.\r\n",
        "interpreter = tf.lite.Interpreter(model_path=\"model.tflite\")\r\n",
        "interpreter.allocate_tensors()\r\n",
        "\r\n",
        "# Get input and output tensors.\r\n",
        "input_details = interpreter.get_input_details()\r\n",
        "output_details = interpreter.get_output_details()\r\n",
        "\r\n",
        "# Test model on random input data.\r\n",
        "input_shape = input_details[0]['shape']\r\n",
        "img = cv2.imread(\"/content/drive/MyDrive/Datasets/DATASET/TEST/tree/00000006.jpg\")\r\n",
        "img = cv2.resize(img, (224,224))\r\n",
        "# print(img)\r\n",
        "print(input_details)\r\n",
        "input_data = np.array(np.expand_dims(img, axis = 0), dtype=np.float32)\r\n",
        "interpreter.set_tensor(input_details[0]['index'], input_data)\r\n",
        "\r\n",
        "interpreter.invoke()\r\n",
        "output_data = interpreter.get_tensor(output_details[0]['index'])\r\n",
        "print(output_data)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[{'name': 'input_19', 'index': 0, 'shape': array([  1, 224, 224,   3], dtype=int32), 'shape_signature': array([ -1, 224, 224,   3], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n",
            "[[2.1032670e-06 1.6492051e-04 2.0010867e-07 9.9901330e-01 8.1944861e-04]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTtK6C2k-Tfu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}